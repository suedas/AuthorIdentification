# -*- coding: utf-8 -*-
"""Metin Madenciligi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Qu-pQPlUPzy6qKT7ZqWnLHPOyRk8EZFB
"""

import pandas
traindf = pandas.read_csv('/content/drive/MyDrive/Metin Madenciliği/train50.csv')
testdf = pandas.read_csv('/content/drive/MyDrive/Metin Madenciliği/test50.csv')
traindf = traindf.sample(frac=1).reset_index(drop=True)#veriler alfabetik olduğu için karıştırma
testdf = testdf.sample(frac=1).reset_index(drop=True)
traindf.head()

traindf.info()

authors = traindf['yazar'].value_counts()
print(authors)
#train veri setinde yazarlara ait satır sayısı

traindf.head()

traindf['text'].dropna(inplace=True)
testdf['text'].dropna(inplace=True)
#boş satırları kaldırma (bu veri setinde boş satır yok)

traindf['text'] = [text.lower() for text in traindf['text']]
testdf['text'] = [text.lower() for text in testdf['text']]
#küçük harfe dönüştürme

import string
#puctuations = !"#$%&'()*+,-./:;<=>?@[\]^_`{|}~
traindf['text'] = traindf['text'].str.replace('[{}]'.format(string.punctuation), '')
testdf['text'] = testdf['text'].str.replace('[{}]'.format(string.punctuation), '')
#noktalama işaretlerinin kaldırılması

traindf['text'] = traindf['text'].apply(lambda x: ' '.join([word for word in x.split() if word.isalpha()]))
testdf['text'] = testdf['text'].apply(lambda x: ' '.join([word for word in x.split() if word.isalpha()]))
#alfabetik karakter içermeyen kelimelerin kaldırılması

import spacy
nlp = spacy.load("en")
stopwords = spacy.lang.en.stop_words.STOP_WORDS
traindf['text'] = traindf['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))
testdf['text'] = testdf['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))
#stop wordsleri kaldırma

"""
from nltk.stem import PorterStemmer
stemmer = PorterStemmer()
traindf['text'] = traindf['text'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))
testdf['text'] = testdf['text'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))
#stemming işlemi
"""

import nltk
nltk.download("wordnet")
lemmatizer = nltk.stem.WordNetLemmatizer()
traindf['text'] = traindf['text'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))
testdf['text'] = testdf['text'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))
#lematizasyon işlemi

traindf.head()

x_train, y_train, x_test, y_test = traindf["text"], traindf["yazar"], testdf["text"], testdf["yazar"]

from sklearn.preprocessing import LabelEncoder
Encoder = LabelEncoder()
y_train = Encoder.fit_transform(y_train)
y_test = Encoder.fit_transform(y_test)

"""
#bag of words
from sklearn.feature_extraction.text import CountVectorizer
bow_transformer = CountVectorizer(binary=True,ngram_range=(1,3),max_df=0.5,stop_words="english").fit(x_train) #
x_train_vector = bow_transformer.transform(x_train)
x_test_vector = bow_transformer.transform(x_test)
print(x_train_vector.shape)
print(x_train_vector)
"""

#tfidf
from sklearn.feature_extraction.text import TfidfVectorizer
tfidf_vect = TfidfVectorizer(ngram_range=(1,3),max_df=0.5,stop_words="english",binary=True).fit(x_train)
x_train_vector = tfidf_vect.transform(x_train)
x_test_vector = tfidf_vect.transform(x_test)
print(x_train_vector.shape)
#print(tfidf_vect.get_feature_names()[882115])
#print(tfidf_vect.get_feature_names()[882114])
print(x_train_vector)

authorss = dict(zip(Encoder.classes_, Encoder.transform(Encoder.classes_)))
print(authorss)

from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(n_estimators=400, random_state=42)
clf.fit(x_train_vector,y_train)
ypred = clf.predict(x_test_vector)

from sklearn.metrics import accuracy_score
print("Accuracy: ",accuracy_score(y_test, ypred))

from sklearn.metrics import classification_report
print(classification_report(y_test,ypred))

from sklearn import svm #Support Vector Machines
SVM = svm.LinearSVC()
SVM.fit(x_train_vector,y_train)
predictions_SVM = SVM.predict(x_test_vector)

print("Accuracy: ",accuracy_score(y_test, predictions_SVM))
print(classification_report(y_test,predictions_SVM))
print(confusion_matrix(y_test, predictions_SVM))

from sklearn.linear_model import SGDClassifier
clf = SGDClassifier()
clf.fit(x_train_vector,y_train)
ypred = clf.predict(x_test_vector)

from sklearn.metrics import accuracy_score
print("Accuracy: ",accuracy_score(y_test, ypred))

from sklearn.metrics import classification_report
print(classification_report(y_test,ypred))

from sklearn.naive_bayes import MultinomialNB #Naive Bayes
Naive = MultinomialNB(alpha=0.005)
Naive.fit(x_train_vector,y_train)
predictions_Naive = Naive.predict(x_test_vector)

from sklearn.metrics import accuracy_score
print("Accuracy: ",accuracy_score(y_test, predictions_Naive))

from sklearn.metrics import classification_report
print(classification_report(y_test,predictions_Naive))

from sklearn.model_selection import GridSearchCV
grid_values = {'alpha':[0.005, 0.01, 0.05, 0.1, 0.5, 1.0]}
grid_nb = GridSearchCV(Naive, param_grid=grid_values, scoring='accuracy')
grid_nb.fit(x_train_vector,y_train)
grid_nb.best_params_
#grid search ile Naive Bayes için kullanılacak olan optimum alpha değerini (laplace smoothing) belirleme

from sklearn.metrics import confusion_matrix
cf_matrix = confusion_matrix(y_test, predictions_Naive)
import matplotlib.pyplot as plt
import seaborn as sns
fig, ax = plt.subplots(figsize=(20,20))
sns.heatmap(cf_matrix, annot=True, ax=ax)
#confusion matrix